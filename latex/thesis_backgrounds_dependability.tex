\section{Improving software dependability}
Dependability (of which security is a special case) has become more and more important of late. The definition of dependability has been given in the introduction, so we will now focus on how to create dependable software. There are many approaches on how to create dependable software; we will list those we consider to be the most important. Please note that the proposed solutions are not mutually exclusive, in fact some of them have already been combined \cite{hohmuth03applying,hunt05singularity}.

\subsection{Safe language}
A lot of software errors are not due to incorrect designs, but due to incorrect programming. The most well known problem is probably that of a \textsl{buffer overflow}. Two of the most used languages, C and C++, offer no inherent protection against buffer overflows and are thus vulnerable to this type of memory error. A solution is to design a language that is memory safe, which means that no buffer overflows can occur. Examples of these languages are C\# and Java. The problem with these languages is that they offer no fine-grained control over memory (de)allocation, which is needed for common performance optimizations to be applied.\emptyline

As maximizing performance is still vital to many programs (such as device drivers, computer games or kernels), these safe languages are often not an option. The approach taken by Cornell University was to develop a dialect of C, named Cyclone \cite{jim02cyclone}, which preserves its syntax and semantics, but also prevents some of the most common errors in the C language (such as the aforementioned buffer overflow) by using new (albeit very similar) syntax.\emptyline

Another approach is to define a completely new, safe language. This approach is taken by the John Hopkins University in their Coyotos project \cite{hopkings06coyotos}. They have developed a safe language, called BitC \cite{shapiro06bitc}, with fully specified semantics. Their approach is directly related to the next solution for a dependable system: code verification.

\subsection{Code verification}
One of the most rigorous methods to improve the dependability of software is to verify its correctness. Verification of code is done by first creating a specification of the code in a verification language and then use that language's verification capabilities to verify the required properties\footnote{An example of such a property is requiring that the program always terminates.}. Examples of verification languages, which are specifically tailored to constructing proofs, are PVS \cite{owre-pvs}, Isabelle/HOL \cite{nipkow02isabelle}, BoogiePL \cite{deline05boogiepl} and Abstract Machine Notation \cite{sorensen01towards}.\emptyline

The problem with specifying code in a verification language is that often no executable code can be generated from it. To still be able to prove properties of executable code, there are two options: convert the verification language to an executable language or vice versa. For both options it is of vital importance that the conversion retains the exact semantics of the source language, otherwise the proofs would not necessarily apply to the corresponding source code or the executable code might not do what the source specification said it would do. The problem with a semantics-retaining conversion is that both languages need to have a clear and well-defined semantics. Unfortunately, the widely used C++ language lacks in this area. Although attempts have been made to develop a clear and well-defined semantics for C++ (which will be discussed later in this chapter), the results so far are still lacking in their applicability to real-world scenarios. The Java language though \textsl{does} have a well-defined semantics; the LOOP compiler \cite{vandenberg01loop} proved that it was feasible to automatically convert Java to a higher-order logic. As a demonstration of its applicability to real-world code, Huisman et. al. used the LOOP compiler to successfully verify a non-trivial property of Java's Vector class \cite{huisman99case}. Unfortunately, the LOOP compiler is not yet generally applicable as it currently does not support threads\footnote{This is especially unfortunate considering the latest trend of developing multi-threaded applications, which is due to multi-processor systems becoming increasingly widespread.}.\emptyline

Even if one has verified properties of source code, it is ultimately the machine code that gets executed. Therefore, it is the machine code that ultimately has to be verified. If one assumes that the conversion from source code to machine code, which is done by the compiler, retains semantics, verification of the source code suffices. Curzon described in \cite{curzon92use} how a compiler can be verified.\emptyline

As said, conversion from certain languages (such as C++) to a verification language has proven to be very hard. An alternative to an automated conversion is to create a model of the source code in the verification language. This has two clear problems; first of all the conversion needs to be done \textit{by-hand} most of the time, which is a far more tedious and error-prone operation than automatic conversion, and secondly there can be no guarantee that proofs in the verification language apply to the source code (as the semantics of the model might not match those of the source code).\emptyline

To assist the prover\footnote{The program with which proofs are constructed in a verification language.} reason about correctness, some languages support the use of source code annotations, which can be used in the verification language. Annotation systems have been developed for a wide variety of languages, such as C \cite{david94lclint}, Java \cite{leavens99jml,hensel98reasoning} and a dialect of C\# called Spec\# \cite{barnett04spec}.

\subsection{Minimized kernel}
The most important part of an operating system (OS) is the kernel, as it handles critical OS parts such as process communication, scheduling and memory management. There are basically two different types of kernels: monolithic- and microkernels. A monolithic kernel executes all its code in the same address space in order to improve performance, whereas a microkernel tries to execute as much of its functionality in user space. A microkernel can be seen as a minimized version of a monolithic kernel. The basic requirements for a microkernel are given in \cite{liedtke95kernel}.\emptyline

Currently two types of microkernels are identified: first generation microkernels such as Mach \cite{rashid89mach} and Minix \cite{herder06minix} and second generation microkernels such as QNX \cite{hildebrand92architectural} and Fiasco \cite{hohmuth03applying}. First generation microkernels still contained code in the kernel which were not strictly necessary for executing its core tasks, second generation microkernels moved this code outside the kernel. Second generation microkernels are thus more \textit{strict} microkernels.\emptyline

Usually, a microkernel's tasks are limited to address space management, thread management and IPC. By restricting its functionality to these core concepts, a microkernel is typically much smaller than a monolithic kernel and thus less likely to contain bugs. Furthermore, the removal of many concepts from the kernel results in a system less likely to crash. Take for example the file server, a typical concept that is included in monolithic kernels (because of its importance on the performance of the kernel), but is not part of a typical microkernel. In the monolithic kernel, the file server has access to all kernel data; a crash of the file server can therefore result in a crash of the whole kernel (and thus everything running on the kernel). However, in the microkernel the file server executes in user space and cannot modify the kernel data. Therefore, a crash of the file server is not likely to crash the microkernel on which it is running.

\subsection{Isolation}
Another way to create a more dependable system is to contain processes in small, isolated spaces. When a process is isolated, it cannot do any harm to other (isolated) processes. There are two types of isolated processes: software isolated processes (SIPs) and hardware isolated processes (HIPs)\footnote{The terms SIP and HIP are taken from \cite{aiken06deconstructing}.}. In practice, many operating systems do not enforce strict process isolation, which results in a system where programs can modify other programs, with potentially very damaging results (for example a single driver's failure crashes the whole system).\emptyline

Hardware isolation of processes can be done by the OS, which limits a processes' memory access to specific pages of physical memory. The OS usually achieves this through standard hardware mechanisms, for example by running user processes in a less priviledged mode than kernel processes\footnote{On x86 processors, processes running at ring 0 are given full control of the processor and lower rings have limited access.}. The mechanism an OS typically uses to hardware-isolate processes is virtual memory. Even though virtual memory is directly supported by most processors, studies show that this form of process isolation is in fact quite costly \cite{aiken06deconstructing,mehnert02cost}.\emptyline

Software isolation depends on the software to isolate processes. The Singularity operating system \cite{hunt05singularity} is designed with software isolation in mind and also heavily uses language safety \cite{aiken06deconstructing,fahndrich06languagesupport,hunt05sealing}. In short, in Singularity each application runs in its own, private software \emph{box}, which ensures that the application can only modify data in its own box.\emptyline

Another form of software isolation is called virtualization. Normally the OS is the lowest software layer on a system, but virtualization adds another layer below the OS. Each virtualized OS thinks it has access to all hardware, but in fact hardware access is controlled by the virtualization layer. The process that manages the virtualization of hardware is often referred to as the \textsl{hypervisor}\footnote{This is a reference to the OS sometimes being referred to as a supervisor. Obviously the virtualization layer has more control than the OS, therefore the term hypervisor as it supersedes supervisor.}. Now several OSes (and its processes) can run simultaneously without interfering with each other (although the hypervisor might allow some limited and controlled communication between OSes), because the hypervisor ensures that an OS cannot access the hardware issued to another OS.\emptyline

Traditionally, virtualization of x86 processors is hard as the x86 architecture does not meet the Popek and Goldberg virtualization requirements \cite{popek74formal}. The main x86 processor manufacturers, AMD and Intel, acknowledged the problems with virtualization and have recently built extensions in their processors to support virtualization\footnote{Respestively called AMD Virtualization \cite{amd05virtual} and Intel Virtualization Technology \cite{intel05virtual}.}. The virtualization layer can now translate the \textit{difficult} OS calls to alternative, virtualization suitable calls, resulting in better virtualization performance.\emptyline

Another way to deal with the \textit{difficult} system calls is to apply paravirtualization. This means that the OS is modified to the extent that no \textit{difficult} calls will be made, therefore no processor-specific virtualization extensions have to be used. Unfortunately, OS modification is not possible for closed-source systems without their creators' explicit consent.