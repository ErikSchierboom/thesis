\hypertarget{discussion}{\chapter{Discussion}}

\begin{epigraphs}
\qitem{``The important thing in science is not so much to obtain new facts as to discover new ways of thinking about them.''}{Sir William Bragg}
\end{epigraphs}

\section{General discussion}
When one wants to attempt verification of a program, that program has to be specified in a language that supports theorem proving. As most programs are written in languages that do not support theorem proving (such as Java and C++), a verification of such a program requires a conversion to a theorem prover (such as PVS). This conversion has to retain the exact semantics of the program, otherwise proofs in the theorem prover do not necessarily apply to the source code.\emptyline

If the semantics of both the program's language and the theorem prover are known, an automatic conversion between these two becomes possible. For larger programs this is an absolute must, as a full, manual conversion would take a huge amount of time. Even small programs can benefit from an automatic conversion as it is not unlikely that an error is introduced in the manual conversion. Unfortunately, for many languages a fully functional conversion is not yet available; most notably this list of languages includes the widely used C++ language. In practice therefore, automatic conversion is not doable for many programs. Even if an automatic conversion would be possible, full verification of programs is (currently) not a realistic option because of the involved complexity.\emptyline

If no automated- or full, manual conversion is possible, the situation is not hopeless though. An alternative solution is to create a model of the program and apply verification to that model. The disadvantage of this method is that it involves a manual conversion and that the proofs cannot be said to directly apply to the program, they are only guaranteed to apply to the model. The proofs are therefore only useful insofar as one trusts the model to be an accurate representation of the source code. However, such a model \textit{can} lead to the detection of flaws in the program design, which we will discuss later. A model also allows for abstraction, which can help to focus on a specific property of the program and usually also lessens the amount of time spent on creating the model and proofs. This approach also opens up the possibility for larger programs to undergo verification, albeit only partial.\emptyline

Even if one manages to formally verify source code, there is often still one vulnerable step remaining: compilation of the source code to machine code. This conversion has to retain the source code's semantics, as otherwise the verified properties of the source code do not necessarily apply to the compiled application.

\section{Research discussion}
Because we worked on C++ code, our options were (at the time) limited to creating a model of the code. However, even if a fully functional C++ to PVS converter had been available, we would probably still have opted for creating a model of the code because of the complexity of the IPC subsystem, which would probably have resulted in very large and complex proofs. This complexity arises because the IPC subsystem is not an isolated subsystem, it has in fact many interdependencies with other parts of the kernel (such as the scheduling subsystem). Therefore, creation of the model was quite tedious, even though the code was fairly well-documented and there was a thesis by Ren\'e Reusner describing the general IPC outline. However, one should take into account though that we had no prior knowledge of the system; had the model been created by someone (intimately) familiar with the source code it would probably have sped up the creation significantly. As creating the model required us to gain a lot of insight in the code, the conversion itself greatly helped in increasing our understanding of the system. We consider this to be an advantage of model creation over automatic conversion, which requires no understanding at all.\emptyline

While creating the model, we experimented with an approach in which we defined a very basic model that could be used and overridden in other theories. Should properties (which are defined in separate theories) require a more detailed model, they could override parts of the basic model and replace them with more detailed versions. The main benefit of this approach is that it results in very minimalistic models: a theory would only expand those parts relevant to its property. This approach would likely have resulted in more compact proofs. Although this approach worked fairly well, there were some disadvantages. As we wanted our model to be a faithful representation of the code, a single, detailed model, which was used in all properties, better reflected the real situation. Another disadvantage was that there was no single point of definition; it was possible to have two theories override parts in conflicting ways. The obvious downside of a single, detailed model was that the proofs became more involved, but the benefit of having a more faithful representation of the code outweighed this disadvantage in our opinion.\emptyline

Creating the model itself was an iterative process, one in which we frequently revisited our design because of slight discrepancies to the code and gained insights into the inner workings of the system. To prevent a single change resulting in a single, large proof having to be redone, we opted for a very modular structure in which lemmas were broken down into smaller lemmas. To better enable the breakdown of lemmas into smaller lemmas, some of the larger functions were split into smaller functions. The original lemmas for the larger functions could now be broken into smaller lemmas specifically targeting the smaller functions. It is important to note that the semantics of the original function needed to be retained when it was split, however this posed no real problems. Through this modular approach we created proofs that were much more resistant to model changes, although some parts of the proof (particularly automatic lemma instantiation) still rendered proofs quite vulnerable to changes.\emptyline

One of the main issues when creating the model was how to model the interruptible nature of Fiasco's IPC implementation. Our solution made use of the fact that the IPC path was only interruptible at specific points in the code (referred to as preemption points). After each preemption point, several actions might have influenced the ongoing IPC (of which we were trying to prove a property). We therefore modelled a preemption point as a function which randomly executed a list of actions that could influence IPC (which included an action where nothing happens). Although this was of great use in our model, it was also a quite risky solution. The main problem is that you have to known \textit{all} actions that can occur in a preemption point along with their effects on IPC. Besides using the source code, we also used the thesis by Ren\'e Reusner \cite{reusner05impl} as a reference, but there is no guarantee that we did not miss any subtle actions (which is not unlikely given the concurrent structure of Fiasco). The obvious solution is to actually model the concurrency, but that would have greatly increased the complexity of the model and the proofs and has therefore been abstracted away.\emptyline

Two of the three lemmas we tried to verify could be proven without resorting to additional assumptions. Although the two verified properties might seem simple, they were both important to the functioning of the system in (very) specific situations. In both cases, we verified that specific threads would not be waiting endlessly in a specific situation, an important property for a real-time kernel. The third property, verification of the assertions in the code, was mainly used to increase confidence in the correctness of our model. It is with this property that we encountered several problems. One problem was the result of our simplified modeling of preemption points, more specifically the modeling of a receiver becoming ready for a sender. When the sender was equal to the receiver, it could become ready for itself which is not possible in the Fiasco IPC implementation we modelled (which included a simplified timeout representation). This however, was not directly stated in the source code but was inferred from it.\emptyline

The second problem we encountered was verifying that at a certain point the sender and receiver were not engaged in IPC. Unfortunately, our verification attempts failed and to continue verification we created a (hopefully temporary) axiom dealing with this assertion. We believe our inability to verify this assumption is in large part due to our simplified model of preemption points, as the assertion mostly deals with the receiver becoming ready which is modelled in the preemption point.\emptyline

The most important problem we found dealt with the \emph{thread\_polling} state bit assertion. In our attempts to verify this specific assertion, we found that we had to assume that initially (before IPC was started) the \emph{thread\_polling} bit was not set. We tried to verify this through adding the assumption as an assertion in the Fiasco source code, which was then recompiled and an IPC test suite was run on it. Unfortunately, this method cannot verify our assumption as we are not sure that it tests the assertion in all possible states. It could however provide a counter-proof to our assumption if the assertion had failed, but it did not fail. As an alternative to this method, we tried to verify that the \emph{thread\_polling} bit being unset was an invariant of IPC; this verification also required the assumption that the \emph{thread\_polling} bit is initially not set. Unfortunately, we failed to verify both the invariant and the original assertion for the exact same reason. The problem was with the \emph{do\_send\_wait()} function, which ought to unset the \emph{thread\_polling} bit before returning. We found that there was a path though in which this was not done. To rule out the most obvious cause of this problem, we checked if the model did not correctly reflect the source code. However, we failed to see any discrepancies between our model and the source code. Not even our simplified preemption point could have been the cause, as the source code made it clear that the \emph{thread\_polling} bit was only changed at very specific functions, all of which were not involved in a preemption point. Our next step was to check with the designer and implementer of the IPC path, Ren{\'e} Reusner, who verified that we had indeed found a bug in the source code (which could be easily fixed). Only when we assumed this bug fixed and used our initial \emph{thread\_polling} state assumption were we able to prove the assertion.\emptyline

We consider the finding of an actual bug in Fiasco's IPC path an important finding. Should the bug occur, it would crash the whole kernel (and thus everything running on top of it). Removal of even one such bug can therefore be considered quite important. It is not odd that the bug had not been found earlier, as it is highly unlikely to occur\footnote{Due to the very short time in which the receiver has time to become ready.}. Although one bug might not seem like much, this can be due to several reasons. One option is that the properties chosen were too simple, they were perhaps not likely to contain any errors at all. Another option is that our model is too abstract or simply incorrect, which might lead to some (lower-level) errors not being found. The last option is that there simply were not many errors in the modelled part of the code. We consider this a very plausible option, as the version of Fiasco we modelled had been extensively tested over a long period of time.\emptyline

Even more important besides finding the bug is the fact that our abstract model was able to find it. This shows that it is not necessary to have a full, one-on-one model in order to find bugs. Please note once again that we have applied a lot of abstractions to our model, an essential component as scheduling has even been completely left out. For larger pieces of software, (partial) verification can thus be a feasible solution when one uses an abstract model. Verification by creating an abstract model can also benefit smaller programs as a more compact model is likely to result in easier, smaller proofs, which can save valuable time. As an abstract model by definition is not completely equal to the source code, there may be subtle errors that are missed though.\emptyline

The choice of abstractions is of the utmost importance as a wrongly chosen abstraction might simplify the model too much and a wrongly defined abstraction can easily lead to incorrect proofs. Defining the abstractions should thus preferrably be done by someone very familiar with the inner workings of the source code. Although the abstractions require a lot of thought, we suggest that the rest of the model is created by keeping the conversion as close to the source code as possible. The main advantage of this method, having a model very faithful to the source code, is increased confidence in the model accurately reflecting the source code. This is important for a model as it inheritly does not directly reflect the source code and thus any proofs of the model are only valid insofar as one trusts the model to be a good reflection of the source code.